# -*- coding: utf-8 -*-
"""Huggingface_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBgwJ6gqqeS02PINr0vljYsWEcTubPQ5
"""

#Huggingface Learning......

#Sentiment Analysis (Positive or Negative)

from transformers import pipeline #use for huggingface

classifier = pipeline("sentiment-analysis")
result = classifier("I was so not happy with the last Mission Impossible Movie")
print(result)

pipeline(task = "sentiment-analysis")("I was confused with the Barbie Movie")

pipeline(task = "sentiment-analysis")\
                                      ("Everyday lots of LLMs papers are published about LLMs Evlauation. \
                                      Lots of them Looks very Promising. \
                                      I am not sure if we CAN actually Evaluate LLMs. \
                                      There is still lots to do.\
                                      Don't you think?")

pipeline(task = "sentiment-analysis", model="facebook/bart-large-mnli")\
                                      ("Everyday lots of LLMs papers are published about LLMs Evlauation. \
                                      Lots of them Looks very Promising. \
                                      I am not sure if we CAN actually Evaluate LLMs. \
                                      There is still lots to do.\
                                      Don't you think?")

classifier = pipeline(task = "sentiment-analysis")

task_list = ["I really like Autoencoders, best models for Anomaly Detection", \
            "I am not sure if we CAN actually Evaluate LLMs.", \
            "PassiveAgressive is the name of a Linear Regression Model that so many people do not know.",\
            "I hate long Meetings."]
classifier(task_list)

classifier = pipeline(task = "sentiment-analysis", model = "SamLowe/roberta-base-go_emotions")

task_list = ["I really like Autoencoders, best models for Anomaly Detection", \
            "I am not sure if we CAN actually Evaluate LLMs.", \
            "PassiveAgressive is the name of a Linear Regression Model that so many people do not know. It is pretty funny name for a Regression Model.",\
            "I hate long Meetings."]
classifier(task_list)

#Text Generation
# Use a pipeline as a high-level helper
from transformers import pipeline

text_generator = pipeline("text-generation", model="distilbert/distilgpt2")
generated_text = text_generator("Today is a rainy day in London",
                                truncation=True,
                                num_return_sequences = 2)
print("Generated_text:\n ", generated_text[0]['generated_text'])

#Question Answering
from transformers import pipeline

qa_model = pipeline("question-answering")
question = "What is my job?"
context = "I am developing Generative AI with Python."
qa_model(question = question, context = context)

# Tokenization
!pip install transformers

from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import pipeline

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenization_model = "nlptown/bert-base-multilingual-uncased-sentiment"

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenization = AutoTokenizer.from_pretrained(tokenization_model)

classifier = pipeline("sentiment-analysis", model=model_name, tokenizer=tokenization)
res = classifier("I was so not happy with the Barbie Movie")
print(res)

from transformers import AutoTokenizer
# Load a pre-trained tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Example text
text = "I was so not happy with the Barbie Movie"

# Tokenize the text
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

# Convert tokens to input IDs
input_ids = tokenizer.convert_tokens_to_ids(tokens)
print("Input IDs:", input_ids)

# Encode the text (tokenization + converting to input IDs)
encoded_input = tokenizer(text)
print("Encoded Input:", encoded_input)

# Decode the text
decoded_output = tokenizer.decode(input_ids)
print("Decode Output: ", decoded_output)

from transformers import AutoTokenizer

# Load a pre-trained tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Example text
text = "I was so not happy with the Barbie Movie"

# Tokenize the text
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

# Convert tokens to input IDs
input_ids = tokenizer.convert_tokens_to_ids(tokens)
print("Input IDs:", input_ids)

# Encode the text (tokenization + converting to input IDs)
encoded_input = tokenizer(text)
print("Encoded Input:", encoded_input)

# Decode the text
decoded_output = tokenizer.decode(input_ids)
print("Decode Output: ", decoded_output)

# Text Summarization using HuggingFace

from transformers import pipeline

# Load summarization pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Example text
text = """
Artificial Intelligence (AI) is transforming industries worldwide.
From healthcare and finance to transportation and education, AI is enabling
automation, efficiency, and innovative solutions. Natural Language Processing (NLP),
a subfield of AI, allows machines to understand, interpret, and generate human language.
This advancement powers applications such as chatbots, virtual assistants,
machine translation, and sentiment analysis.
Despite its benefits, AI also raises ethical concerns about bias, privacy, and job displacement.
"""

# Generate summary
summary = summarizer(text, max_length=80, min_length=30, do_sample=False)

print("Original Text:\n", text)
print("\nSummary:\n", summary[0]['summary_text'])

from transformers import pipeline
generator = pipeline("text2text-generation", model="t5-small")
translation = generator("translate English to German: How are you?", max_length=40)
print("\nTranslation (ENâ†’DE):\n", translation[0]['generated_text'])

# Fill-Mask using HuggingFace
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")

result = unmasker("I love to play [MASK] in the evening.")
for r in result:
    print(f"{r['sequence']} (score: {r['score']:.4f})")